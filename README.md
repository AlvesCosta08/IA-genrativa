ğŸ“„ Dr. Legal & Advogados â€“ Assistente JurÃ­dico Virtual
Um sistema completo de atendimento jurÃ­dico automatizado com chatbot, IA local e integraÃ§Ã£o web.
Desenvolvido com Flask, Ollama e Docker para privacidade, desempenho e conversÃ£o. 

ğŸŒŸ VisÃ£o Geral
O Dr. Legal & Advogados Ã© um site institucional com um assistente jurÃ­dico virtual inteligente, capaz de:

Entender dÃºvidas jurÃ­dicas em linguagem natural.
Responder com empatia e clareza.
Detectar a Ã¡rea do direito mais adequada (FamÃ­lia, Trabalhista, PrevidenciÃ¡rio, etc.).
Direcionar o usuÃ¡rio para um advogado real via WhatsApp com mensagens personalizadas.
Todo o processamento de IA Ã© feito localmente com Ollama, garantindo privacidade e baixa latÃªncia.

ğŸ”§ Arquitetura do Sistema
O sistema Ã© composto por dois serviÃ§os Docker:

ollama
Ollama + Ubuntu
Executa modelos de IA localmente (ex:
tinyllama
)
web
Flask + Python
Backend do chatbot e frontend do site

Eles sÃ£o orquestrados com Docker Compose, garantindo fÃ¡cil implantaÃ§Ã£o.


projeto/
â”‚
â”œâ”€â”€ ollama-container/
â”‚   â”œâ”€â”€ Dockerfile         # âœ”ï¸ (jÃ¡ existente)
â”‚   â””â”€â”€ start.sh           # âœ”ï¸ (jÃ¡ existente)
â”‚
â”œâ”€â”€ web-container/
â”‚   â”œâ”€â”€ Dockerfile         # âœ… 
â”‚   â”œâ”€â”€ app.py             # âœ”ï¸ (backend Flask)
â”‚   â”œâ”€â”€ requirements.txt   # âœ”ï¸ (com flask e requests)
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html     # âœ”ï¸ (frontend completo com chatbot)
â”‚
â”œâ”€â”€ docker-compose.yml     # âœ”ï¸ (orquestraÃ§Ã£o dos serviÃ§os)
â””â”€â”€ README.md              # âœ”ï¸ (documentaÃ§Ã£o completa)


âš™ï¸ ServiÃ§o 1: ollama-container (IA Local)

FROM ubuntu:22.04

# Evita perguntas durante instalaÃ§Ã£o
ENV DEBIAN_FRONTEND=noninteractive

# Instala curl e wget
RUN apt update && \
    apt install -y curl wget sudo && \
    rm -rf /var/lib/apt/lists/*

# Instala o Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Cria usuÃ¡rio ollama
RUN useradd -m -s /bin/bash ollama && \
    usermod -aG ollama ollama && \
    echo "ollama ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Permite acesso externo
ENV OLLAMA_HOST=0.0.0.0

# DiretÃ³rio do Ollama
VOLUME ["/root/.ollama"]

# Porta da API
EXPOSE 11434

# Script de inicializaÃ§Ã£o
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Inicia o Ollama
CMD ["/start.sh"]


#!/bin/bash
echo "ğŸš€ Iniciando Ollama..."
ollama serve &

sleep 10

echo "ğŸ“¥ Baixando modelo ultra leve: tinyllama:1.1b"
ollama pull tinyllama:1.1b

echo "âœ… Modelo pronto!"
wait

O que faz?
Instala o Ollama no Ubuntu.
Inicia o servidor ollama serve.
Baixa automaticamente o modelo leve TinyLlama (1.1B) para inferÃªncia local.
ExpÃµe a API em http://localhost:11434.
Usa volume Docker para nÃ£o baixar o modelo novamente em reinicializaÃ§Ãµes.
âœ… O script oficial do Ollama detecta automaticamente GPU (NVIDIA/AMD) e instala drivers se necessÃ¡rio. 


FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]

profile
Qwen3-235B-A22B-2507
8:42 pm
Claro! Abaixo estÃ¡ o conteÃºdo completo do README.md em um Ãºnico arquivo, com todas as informaÃ§Ãµes integradas: cÃ³digo, arquitetura, funcionalidades, frontend, backend, Docker, IA local, conversÃ£o e muito mais. VocÃª pode copiar e salvar como README.md.

ğŸ“„ Dr. Legal & Advogados â€“ Assistente JurÃ­dico Virtual
Um sistema completo de atendimento jurÃ­dico automatizado com chatbot, IA local e integraÃ§Ã£o web.
Desenvolvido com Flask, Ollama e Docker para privacidade, desempenho e conversÃ£o. 

ğŸŒŸ VisÃ£o Geral
O Dr. Legal & Advogados Ã© um site institucional com um assistente jurÃ­dico virtual inteligente, capaz de:

Entender dÃºvidas jurÃ­dicas em linguagem natural.
Responder com empatia e clareza.
Detectar a Ã¡rea do direito mais adequada (FamÃ­lia, Trabalhista, PrevidenciÃ¡rio, etc.).
Direcionar o usuÃ¡rio para um advogado real via WhatsApp com mensagens personalizadas.
Todo o processamento de IA Ã© feito localmente com Ollama, garantindo privacidade, conformidade com a LGPD e baixa latÃªncia.

ğŸ”§ Arquitetura do Sistema
O sistema Ã© composto por dois serviÃ§os Docker:

ollama
Ollama + Ubuntu
Executa modelos de IA localmente (ex:
tinyllama
)
web
Flask + Python
Backend do chatbot e frontend do site

Eles sÃ£o orquestrados com Docker Compose, garantindo fÃ¡cil implantaÃ§Ã£o e escalabilidade.

ğŸ“¦ Estrutura de Pastas


1
2
3
4
5
6
7
8
9
10
11
12
13
14
projeto/
â”‚
â”œâ”€â”€ ollama-container/
â”‚   â”œâ”€â”€ Dockerfile         # Cria o container do Ollama
â”‚   â””â”€â”€ start.sh           # Inicia o Ollama e baixa o modelo
â”‚
â”œâ”€â”€ web-container/
â”‚   â”œâ”€â”€ app.py             # Backend Flask do chatbot
â”‚   â”œâ”€â”€ requirements.txt   # DependÃªncias Python
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html     # Frontend do site
â”‚
â”œâ”€â”€ docker-compose.yml     # OrquestraÃ§Ã£o dos serviÃ§os
â””â”€â”€ README.md              # Este arquivo
âš™ï¸ ServiÃ§o 1: ollama-container (IA Local)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
FROM ubuntu:22.04

# Evita perguntas durante instalaÃ§Ã£o
ENV DEBIAN_FRONTEND=noninteractive

# Instala curl e wget
RUN apt update && \
    apt install -y curl wget sudo && \
    rm -rf /var/lib/apt/lists/*

# Instala o Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Cria usuÃ¡rio ollama
RUN useradd -m -s /bin/bash ollama && \
    usermod -aG ollama ollama && \
    echo "ollama ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Permite acesso externo
ENV OLLAMA_HOST=0.0.0.0

# DiretÃ³rio do Ollama
VOLUME ["/root/.ollama"]

# Porta da API
EXPOSE 11434

# Script de inicializaÃ§Ã£o
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Inicia o Ollama
CMD ["/start.sh"]
start.sh
bash


1
2
3
4
5
6
7
8
9
10
11
#!/bin/bash
echo "ğŸš€ Iniciando Ollama..."
ollama serve &

sleep 10

echo "ğŸ“¥ Baixando modelo ultra leve: tinyllama:1.1b"
ollama pull tinyllama:1.1b

echo "âœ… Modelo pronto!"
wait
ğŸ” O que faz?
Instala o Ollama no Ubuntu.
Inicia o servidor ollama serve.
Baixa automaticamente o modelo leve TinyLlama (1.1B) para inferÃªncia local.
ExpÃµe a API em http://localhost:11434.
Usa volume Docker para nÃ£o baixar o modelo novamente em reinicializaÃ§Ãµes.
âœ… O script oficial do Ollama detecta automaticamente GPU (NVIDIA/AMD) e instala drivers se necessÃ¡rio. 

ğŸŒ ServiÃ§o 2: web-container (Backend + Frontend)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]
requirements.txt
txt


1
2
flask
requests
app.py â€“ Backend Flask
Este script:

Detecta temas jurÃ­dicos com base em palavras-chave (ex: "divÃ³rcio", "FGTS", "aposentadoria").
Classifica a Ã¡rea do direito mais relevante.
Consulta o modelo local tinyllama via API do Ollama.
Gera respostas humanizadas com chamadas para aÃ§Ã£o (CTA).
Insere botÃµes do WhatsApp para conversÃ£o.
Redireciona sempre para contato humano â€“ nunca diz "nÃ£o posso ajudar".
Exemplo de CTA gerado:

<a href="https://wa.me/551199887766?text=Quero+falar+sobre+divÃ³rcio">
  ğŸ“ Falar com especialista em Direito de FamÃ­lia
</a>

ğŸ’¬ Funcionalidades do Chatbot (app.py)
O backend app.py possui mÃºltiplas camadas de inteligÃªncia:

1. DetecÃ§Ã£o de IntenÃ§Ã£o
SaudaÃ§Ãµes, despedidas, contato, horÃ¡rios, honorÃ¡rios.
Temas comuns como "divÃ³rcio", "acidente", "golpe no PIX".
BenefÃ­cios previdenciÃ¡rios com respostas especÃ­ficas.
2. Palavras-Chave por Ãrea


3. IA com Foco em ConversÃ£o
Prompt otimizado para empathia e aÃ§Ã£o.
Nunca responde como robÃ´.
Sempre termina com CTA para WhatsApp.
4. Redirecionamento Garantido
Mesmo se a IA falhar, o usuÃ¡rio Ã© direcionado ao WhatsApp.
Nenhum caso Ã© perdido.
ğŸ–¥ï¸ Frontend (index.html)
CaracterÃ­sticas
Design moderno com Bootstrap 5 e Font Awesome.
Tipografia elegante com Google Fonts (Inter + Playfair Display).
Chatbot fixo no canto inferior direito com animaÃ§Ã£o de pulsaÃ§Ã£o.
Totalmente responsivo (mÃ³vel e desktop).
Recursos do Chat
Abre com clique no botÃ£o flutuante.
Interface limpa com mensagens do usuÃ¡rio (azul) e IA (cinza).
Mostra "digitando..." enquanto aguarda resposta.
Envia mensagem com Enter.
ComunicaÃ§Ã£o com Backend

fetch('http://localhost:5000/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ pergunta: msg })
})

ğŸ³ Docker Compose
docker-compose.yml

services:
  ollama:
    build: ./ollama-container
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  web:
    build: ./web-container
    ports:
      - "5000:5000"
    depends_on:
      - ollama

volumes:
  ollama_data:

  profile
Qwen3-235B-A22B-2507
8:42 pm
Claro! Abaixo estÃ¡ o conteÃºdo completo do README.md em um Ãºnico arquivo, com todas as informaÃ§Ãµes integradas: cÃ³digo, arquitetura, funcionalidades, frontend, backend, Docker, IA local, conversÃ£o e muito mais. VocÃª pode copiar e salvar como README.md.

ğŸ“„ Dr. Legal & Advogados â€“ Assistente JurÃ­dico Virtual
Um sistema completo de atendimento jurÃ­dico automatizado com chatbot, IA local e integraÃ§Ã£o web.
Desenvolvido com Flask, Ollama e Docker para privacidade, desempenho e conversÃ£o. 

ğŸŒŸ VisÃ£o Geral
O Dr. Legal & Advogados Ã© um site institucional com um assistente jurÃ­dico virtual inteligente, capaz de:

Entender dÃºvidas jurÃ­dicas em linguagem natural.
Responder com empatia e clareza.
Detectar a Ã¡rea do direito mais adequada (FamÃ­lia, Trabalhista, PrevidenciÃ¡rio, etc.).
Direcionar o usuÃ¡rio para um advogado real via WhatsApp com mensagens personalizadas.
Todo o processamento de IA Ã© feito localmente com Ollama, garantindo privacidade, conformidade com a LGPD e baixa latÃªncia.

ğŸ”§ Arquitetura do Sistema
O sistema Ã© composto por dois serviÃ§os Docker:

ollama
Ollama + Ubuntu
Executa modelos de IA localmente (ex:
tinyllama
)
web
Flask + Python
Backend do chatbot e frontend do site

Eles sÃ£o orquestrados com Docker Compose, garantindo fÃ¡cil implantaÃ§Ã£o e escalabilidade.

ğŸ“¦ Estrutura de Pastas


1
2
3
4
5
6
7
8
9
10
11
12
13
14
projeto/
â”‚
â”œâ”€â”€ ollama-container/
â”‚   â”œâ”€â”€ Dockerfile         # Cria o container do Ollama
â”‚   â””â”€â”€ start.sh           # Inicia o Ollama e baixa o modelo
â”‚
â”œâ”€â”€ web-container/
â”‚   â”œâ”€â”€ app.py             # Backend Flask do chatbot
â”‚   â”œâ”€â”€ requirements.txt   # DependÃªncias Python
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html     # Frontend do site
â”‚
â”œâ”€â”€ docker-compose.yml     # OrquestraÃ§Ã£o dos serviÃ§os
â””â”€â”€ README.md              # Este arquivo
âš™ï¸ ServiÃ§o 1: ollama-container (IA Local)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
FROM ubuntu:22.04

# Evita perguntas durante instalaÃ§Ã£o
ENV DEBIAN_FRONTEND=noninteractive

# Instala curl e wget
RUN apt update && \
    apt install -y curl wget sudo && \
    rm -rf /var/lib/apt/lists/*

# Instala o Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Cria usuÃ¡rio ollama
RUN useradd -m -s /bin/bash ollama && \
    usermod -aG ollama ollama && \
    echo "ollama ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Permite acesso externo
ENV OLLAMA_HOST=0.0.0.0

# DiretÃ³rio do Ollama
VOLUME ["/root/.ollama"]

# Porta da API
EXPOSE 11434

# Script de inicializaÃ§Ã£o
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Inicia o Ollama
CMD ["/start.sh"]
start.sh
bash


1
2
3
4
5
6
7
8
9
10
11
#!/bin/bash
echo "ğŸš€ Iniciando Ollama..."
ollama serve &

sleep 10

echo "ğŸ“¥ Baixando modelo ultra leve: tinyllama:1.1b"
ollama pull tinyllama:1.1b

echo "âœ… Modelo pronto!"
wait
ğŸ” O que faz?
Instala o Ollama no Ubuntu.
Inicia o servidor ollama serve.
Baixa automaticamente o modelo leve TinyLlama (1.1B) para inferÃªncia local.
ExpÃµe a API em http://localhost:11434.
Usa volume Docker para nÃ£o baixar o modelo novamente em reinicializaÃ§Ãµes.
âœ… O script oficial do Ollama detecta automaticamente GPU (NVIDIA/AMD) e instala drivers se necessÃ¡rio. 

ğŸŒ ServiÃ§o 2: web-container (Backend + Frontend)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]
requirements.txt
txt


1
2
flask
requests
app.py â€“ Backend Flask
Este script:

Detecta temas jurÃ­dicos com base em palavras-chave (ex: "divÃ³rcio", "FGTS", "aposentadoria").
Classifica a Ã¡rea do direito mais relevante.
Consulta o modelo local tinyllama via API do Ollama.
Gera respostas humanizadas com chamadas para aÃ§Ã£o (CTA).
Insere botÃµes do WhatsApp para conversÃ£o.
Redireciona sempre para contato humano â€“ nunca diz "nÃ£o posso ajudar".
Exemplo de CTA gerado:
html


1
2
3
âŒ„
<a href="https://wa.me/551199887766?text=Quero+falar+sobre+divÃ³rcio">
  ğŸ“ Falar com especialista em Direito de FamÃ­lia
</a>
ğŸ’¬ Funcionalidades do Chatbot (app.py)
O backend app.py possui mÃºltiplas camadas de inteligÃªncia:

1. DetecÃ§Ã£o de IntenÃ§Ã£o
SaudaÃ§Ãµes, despedidas, contato, horÃ¡rios, honorÃ¡rios.
Temas comuns como "divÃ³rcio", "acidente", "golpe no PIX".
BenefÃ­cios previdenciÃ¡rios com respostas especÃ­ficas.
2. Palavras-Chave por Ãrea
python


1
2
3
4
5
6
âŒ„
PALAVRAS_JURIDICAS = {
    "Direito de FamÃ­lia": ["divÃ³rcio", "guarda", "pensÃ£o", ...],
    "Direito Trabalhista": ["demitido", "horas extras", "FGTS", ...],
    "Direito PrevidenciÃ¡rio": ["aposentadoria", "auxÃ­lio-doenÃ§a", ...],
    # + outras Ã¡reas
}
3. IA com Foco em ConversÃ£o
Prompt otimizado para empathia e aÃ§Ã£o.
Nunca responde como robÃ´.
Sempre termina com CTA para WhatsApp.
4. Redirecionamento Garantido
Mesmo se a IA falhar, o usuÃ¡rio Ã© direcionado ao WhatsApp.
Nenhum caso Ã© perdido.
ğŸ–¥ï¸ Frontend (index.html)
CaracterÃ­sticas
Design moderno com Bootstrap 5 e Font Awesome.
Tipografia elegante com Google Fonts (Inter + Playfair Display).
Chatbot fixo no canto inferior direito com animaÃ§Ã£o de pulsaÃ§Ã£o.
Totalmente responsivo (mÃ³vel e desktop).
Recursos do Chat
Abre com clique no botÃ£o flutuante.
Interface limpa com mensagens do usuÃ¡rio (azul) e IA (cinza).
Mostra "digitando..." enquanto aguarda resposta.
Envia mensagem com Enter.
ComunicaÃ§Ã£o com Backend
javascript


1
2
3
4
5
âŒ„
fetch('http://localhost:5000/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ pergunta: msg })
})
ğŸ³ Docker Compose
docker-compose.yml
yaml


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
âŒ„
services:
  ollama:
    build: ./ollama-container
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  web:
    build: ./web-container
    ports:
      - "5000:5000"
    depends_on:
      - ollama

volumes:
  ollama_data:
Funcionalidades:
O serviÃ§o web sÃ³ inicia apÃ³s o ollama estar pronto.
Dados do Ollama sÃ£o persistidos no volume ollama_data.
Portas expostas: 5000 (site) e 11434 (API da IA).
ğŸš€ Como Executar
Clone o repositÃ³rio
Certifique-se de ter Docker e Docker Compose instalados
Execute:

docker-compose up --build

Acesse:
Site: http://localhost:5000
API da IA: http://localhost:11434
ğŸ’¡ Recursos de ConversÃ£o
O sistema foi projetado para maximizar conversÃµes:

Primeira consulta gratuita
Reduz barreira inicial
BotÃµes do WhatsApp
ConversÃ£o direta
CTAs personalizados por Ã¡rea
Maior relevÃ¢ncia
PlantÃ£o 24h
UrgÃªncia
Respostas curtas e humanas
Clareza e empatia
Redirecionamento para humano
Nenhum caso perdido

ğŸ›¡ï¸ Privacidade e SeguranÃ§a
Nenhum dado do usuÃ¡rio Ã© armazenado.
Toda IA roda localmente (sem envio a nuvens como OpenAI).
Sem cookies de rastreamento.
Ideal para advocacia: cumple LGPD e Ã©tica profissional.
ğŸ§  Modelo de IA Utilizado
tinyllama:1.1b
Pequeno, rÃ¡pido e eficiente.
Roda bem em CPU (ideal para produÃ§Ã£o).
Treinado em linguagem natural.
Adaptado via prompt para responder como um advogado empÃ¡tico.
âœ… Futuramente pode ser substituÃ­do por modelos maiores como llama3, phi3 ou gemma com melhor desempenho. 

ğŸ“ˆ Fluxo de Atendimento
UsuÃ¡rio digita: "CaÃ­ em golpe no PIX, o que fazer?"
Sistema detecta palavra-chave â†’ "golpe", "PIX"
Classifica como: Direito do Consumidor
Gera resposta com CTA:
"Se vocÃª foi enganado no PIX, podemos tentar recuperar seu dinheiro.
ğŸ“ Falar com especialista em Consumidor " 
UsuÃ¡rio clica e conversa com advogado real.
ğŸ“ Contato
Este projeto foi desenvolvido para escritÃ³rios de advocacia que desejam:

Atendimento 24h
ReduÃ§Ã£o de custos
Aumento de conversÃ£o
Entre em contato:
ğŸ“§ contato@drlegal.com.br
ğŸ“ (11) 99999-9999

ğŸ“„ LicenÃ§a
Este projeto Ã© um exemplo educacional.
VocÃª pode adaptÃ¡-lo livremente para uso comercial ou institucional.