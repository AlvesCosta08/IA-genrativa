üìÑ Dr. Legal & Advogados ‚Äì Assistente Jur√≠dico Virtual
Um sistema completo de atendimento jur√≠dico automatizado com chatbot, IA local e integra√ß√£o web.
Desenvolvido com Flask, Ollama e Docker para privacidade, desempenho e convers√£o. 

üåü Vis√£o Geral
O Dr. Legal & Advogados √© um site institucional com um assistente jur√≠dico virtual inteligente, capaz de:

Entender d√∫vidas jur√≠dicas em linguagem natural.
Responder com empatia e clareza.
Detectar a √°rea do direito mais adequada (Fam√≠lia, Trabalhista, Previdenci√°rio, etc.).
Direcionar o usu√°rio para um advogado real via WhatsApp com mensagens personalizadas.
Todo o processamento de IA √© feito localmente com Ollama, garantindo privacidade e baixa lat√™ncia.

üîß Arquitetura do Sistema
O sistema √© composto por dois servi√ßos Docker:

ollama
Ollama + Ubuntu
Executa modelos de IA localmente (ex:
tinyllama
)
web
Flask + Python
Backend do chatbot e frontend do site

Eles s√£o orquestrados com Docker Compose, garantindo f√°cil implanta√ß√£o.


projeto/
‚îÇ
‚îú‚îÄ‚îÄ ollama-container/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile         # ‚úîÔ∏è (j√° existente)
‚îÇ   ‚îî‚îÄ‚îÄ start.sh           # ‚úîÔ∏è (j√° existente)
‚îÇ
‚îú‚îÄ‚îÄ web-container/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile         # ‚úÖ 
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # ‚úîÔ∏è (backend Flask)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt   # ‚úîÔ∏è (com flask e requests)
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ index.html     # ‚úîÔ∏è (frontend completo com chatbot)
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml     # ‚úîÔ∏è (orquestra√ß√£o dos servi√ßos)
‚îî‚îÄ‚îÄ README.md              # ‚úîÔ∏è (documenta√ß√£o completa)


‚öôÔ∏è Servi√ßo 1: ollama-container (IA Local)

FROM ubuntu:22.04

# Evita perguntas durante instala√ß√£o
ENV DEBIAN_FRONTEND=noninteractive

# Instala curl e wget
RUN apt update && \
    apt install -y curl wget sudo && \
    rm -rf /var/lib/apt/lists/*

# Instala o Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Cria usu√°rio ollama
RUN useradd -m -s /bin/bash ollama && \
    usermod -aG ollama ollama && \
    echo "ollama ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Permite acesso externo
ENV OLLAMA_HOST=0.0.0.0

# Diret√≥rio do Ollama
VOLUME ["/root/.ollama"]

# Porta da API
EXPOSE 11434

# Script de inicializa√ß√£o
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Inicia o Ollama
CMD ["/start.sh"]


#!/bin/bash
echo "üöÄ Iniciando Ollama..."
ollama serve &

sleep 10

echo "üì• Baixando modelo ultra leve: tinyllama:1.1b"
ollama pull tinyllama:1.1b

echo "‚úÖ Modelo pronto!"
wait

O que faz?
Instala o Ollama no Ubuntu.
Inicia o servidor ollama serve.
Baixa automaticamente o modelo leve TinyLlama (1.1B) para infer√™ncia local.
Exp√µe a API em http://localhost:11434.
Usa volume Docker para n√£o baixar o modelo novamente em reinicializa√ß√µes.
‚úÖ O script oficial do Ollama detecta automaticamente GPU (NVIDIA/AMD) e instala drivers se necess√°rio. 


FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]

profile
Qwen3-235B-A22B-2507
8:42 pm
Claro! Abaixo est√° o conte√∫do completo do README.md em um √∫nico arquivo, com todas as informa√ß√µes integradas: c√≥digo, arquitetura, funcionalidades, frontend, backend, Docker, IA local, convers√£o e muito mais. Voc√™ pode copiar e salvar como README.md.

üìÑ Dr. Legal & Advogados ‚Äì Assistente Jur√≠dico Virtual
Um sistema completo de atendimento jur√≠dico automatizado com chatbot, IA local e integra√ß√£o web.
Desenvolvido com Flask, Ollama e Docker para privacidade, desempenho e convers√£o. 

üåü Vis√£o Geral
O Dr. Legal & Advogados √© um site institucional com um assistente jur√≠dico virtual inteligente, capaz de:

Entender d√∫vidas jur√≠dicas em linguagem natural.
Responder com empatia e clareza.
Detectar a √°rea do direito mais adequada (Fam√≠lia, Trabalhista, Previdenci√°rio, etc.).
Direcionar o usu√°rio para um advogado real via WhatsApp com mensagens personalizadas.
Todo o processamento de IA √© feito localmente com Ollama, garantindo privacidade, conformidade com a LGPD e baixa lat√™ncia.

üîß Arquitetura do Sistema
O sistema √© composto por dois servi√ßos Docker:

ollama
Ollama + Ubuntu
Executa modelos de IA localmente (ex:
tinyllama
)
web
Flask + Python
Backend do chatbot e frontend do site

Eles s√£o orquestrados com Docker Compose, garantindo f√°cil implanta√ß√£o e escalabilidade.

üì¶ Estrutura de Pastas


1
2
3
4
5
6
7
8
9
10
11
12
13
14
projeto/
‚îÇ
‚îú‚îÄ‚îÄ ollama-container/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile         # Cria o container do Ollama
‚îÇ   ‚îî‚îÄ‚îÄ start.sh           # Inicia o Ollama e baixa o modelo
‚îÇ
‚îú‚îÄ‚îÄ web-container/
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Backend Flask do chatbot
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt   # Depend√™ncias Python
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ index.html     # Frontend do site
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml     # Orquestra√ß√£o dos servi√ßos
‚îî‚îÄ‚îÄ README.md              # Este arquivo
‚öôÔ∏è Servi√ßo 1: ollama-container (IA Local)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
FROM ubuntu:22.04

# Evita perguntas durante instala√ß√£o
ENV DEBIAN_FRONTEND=noninteractive

# Instala curl e wget
RUN apt update && \
    apt install -y curl wget sudo && \
    rm -rf /var/lib/apt/lists/*

# Instala o Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Cria usu√°rio ollama
RUN useradd -m -s /bin/bash ollama && \
    usermod -aG ollama ollama && \
    echo "ollama ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Permite acesso externo
ENV OLLAMA_HOST=0.0.0.0

# Diret√≥rio do Ollama
VOLUME ["/root/.ollama"]

# Porta da API
EXPOSE 11434

# Script de inicializa√ß√£o
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Inicia o Ollama
CMD ["/start.sh"]
start.sh
bash


1
2
3
4
5
6
7
8
9
10
11
#!/bin/bash
echo "üöÄ Iniciando Ollama..."
ollama serve &

sleep 10

echo "üì• Baixando modelo ultra leve: tinyllama:1.1b"
ollama pull tinyllama:1.1b

echo "‚úÖ Modelo pronto!"
wait
üîç O que faz?
Instala o Ollama no Ubuntu.
Inicia o servidor ollama serve.
Baixa automaticamente o modelo leve TinyLlama (1.1B) para infer√™ncia local.
Exp√µe a API em http://localhost:11434.
Usa volume Docker para n√£o baixar o modelo novamente em reinicializa√ß√µes.
‚úÖ O script oficial do Ollama detecta automaticamente GPU (NVIDIA/AMD) e instala drivers se necess√°rio. 

üåê Servi√ßo 2: web-container (Backend + Frontend)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]
requirements.txt
txt


1
2
flask
requests
app.py ‚Äì Backend Flask
Este script:

Detecta temas jur√≠dicos com base em palavras-chave (ex: "div√≥rcio", "FGTS", "aposentadoria").
Classifica a √°rea do direito mais relevante.
Consulta o modelo local tinyllama via API do Ollama.
Gera respostas humanizadas com chamadas para a√ß√£o (CTA).
Insere bot√µes do WhatsApp para convers√£o.
Redireciona sempre para contato humano ‚Äì nunca diz "n√£o posso ajudar".
Exemplo de CTA gerado:

<a href="https://wa.me/551199887766?text=Quero+falar+sobre+div√≥rcio">
  üìû Falar com especialista em Direito de Fam√≠lia
</a>

üí¨ Funcionalidades do Chatbot (app.py)
O backend app.py possui m√∫ltiplas camadas de intelig√™ncia:

1. Detec√ß√£o de Inten√ß√£o
Sauda√ß√µes, despedidas, contato, hor√°rios, honor√°rios.
Temas comuns como "div√≥rcio", "acidente", "golpe no PIX".
Benef√≠cios previdenci√°rios com respostas espec√≠ficas.
2. Palavras-Chave por √Årea


3. IA com Foco em Convers√£o
Prompt otimizado para empathia e a√ß√£o.
Nunca responde como rob√¥.
Sempre termina com CTA para WhatsApp.
4. Redirecionamento Garantido
Mesmo se a IA falhar, o usu√°rio √© direcionado ao WhatsApp.
Nenhum caso √© perdido.
üñ•Ô∏è Frontend (index.html)
Caracter√≠sticas
Design moderno com Bootstrap 5 e Font Awesome.
Tipografia elegante com Google Fonts (Inter + Playfair Display).
Chatbot fixo no canto inferior direito com anima√ß√£o de pulsa√ß√£o.
Totalmente responsivo (m√≥vel e desktop).
Recursos do Chat
Abre com clique no bot√£o flutuante.
Interface limpa com mensagens do usu√°rio (azul) e IA (cinza).
Mostra "digitando..." enquanto aguarda resposta.
Envia mensagem com Enter.
Comunica√ß√£o com Backend

fetch('http://localhost:5000/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ pergunta: msg })
})

üê≥ Docker Compose
docker-compose.yml

services:
  ollama:
    build: ./ollama-container
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  web:
    build: ./web-container
    ports:
      - "5000:5000"
    depends_on:
      - ollama

volumes:
  ollama_data:

  profile
Qwen3-235B-A22B-2507
8:42 pm
Claro! Abaixo est√° o conte√∫do completo do README.md em um √∫nico arquivo, com todas as informa√ß√µes integradas: c√≥digo, arquitetura, funcionalidades, frontend, backend, Docker, IA local, convers√£o e muito mais. Voc√™ pode copiar e salvar como README.md.

üìÑ Dr. Legal & Advogados ‚Äì Assistente Jur√≠dico Virtual
Um sistema completo de atendimento jur√≠dico automatizado com chatbot, IA local e integra√ß√£o web.
Desenvolvido com Flask, Ollama e Docker para privacidade, desempenho e convers√£o. 

üåü Vis√£o Geral
O Dr. Legal & Advogados √© um site institucional com um assistente jur√≠dico virtual inteligente, capaz de:

Entender d√∫vidas jur√≠dicas em linguagem natural.
Responder com empatia e clareza.
Detectar a √°rea do direito mais adequada (Fam√≠lia, Trabalhista, Previdenci√°rio, etc.).
Direcionar o usu√°rio para um advogado real via WhatsApp com mensagens personalizadas.
Todo o processamento de IA √© feito localmente com Ollama, garantindo privacidade, conformidade com a LGPD e baixa lat√™ncia.

üîß Arquitetura do Sistema
O sistema √© composto por dois servi√ßos Docker:

ollama
Ollama + Ubuntu
Executa modelos de IA localmente (ex:
tinyllama
)
web
Flask + Python
Backend do chatbot e frontend do site

Eles s√£o orquestrados com Docker Compose, garantindo f√°cil implanta√ß√£o e escalabilidade.

üì¶ Estrutura de Pastas


1
2
3
4
5
6
7
8
9
10
11
12
13
14
projeto/
‚îÇ
‚îú‚îÄ‚îÄ ollama-container/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile         # Cria o container do Ollama
‚îÇ   ‚îî‚îÄ‚îÄ start.sh           # Inicia o Ollama e baixa o modelo
‚îÇ
‚îú‚îÄ‚îÄ web-container/
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Backend Flask do chatbot
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt   # Depend√™ncias Python
‚îÇ   ‚îî‚îÄ‚îÄ templates/
‚îÇ       ‚îî‚îÄ‚îÄ index.html     # Frontend do site
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml     # Orquestra√ß√£o dos servi√ßos
‚îî‚îÄ‚îÄ README.md              # Este arquivo
‚öôÔ∏è Servi√ßo 1: ollama-container (IA Local)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
FROM ubuntu:22.04

# Evita perguntas durante instala√ß√£o
ENV DEBIAN_FRONTEND=noninteractive

# Instala curl e wget
RUN apt update && \
    apt install -y curl wget sudo && \
    rm -rf /var/lib/apt/lists/*

# Instala o Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Cria usu√°rio ollama
RUN useradd -m -s /bin/bash ollama && \
    usermod -aG ollama ollama && \
    echo "ollama ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Permite acesso externo
ENV OLLAMA_HOST=0.0.0.0

# Diret√≥rio do Ollama
VOLUME ["/root/.ollama"]

# Porta da API
EXPOSE 11434

# Script de inicializa√ß√£o
COPY start.sh /start.sh
RUN chmod +x /start.sh

# Inicia o Ollama
CMD ["/start.sh"]
start.sh
bash


1
2
3
4
5
6
7
8
9
10
11
#!/bin/bash
echo "üöÄ Iniciando Ollama..."
ollama serve &

sleep 10

echo "üì• Baixando modelo ultra leve: tinyllama:1.1b"
ollama pull tinyllama:1.1b

echo "‚úÖ Modelo pronto!"
wait
üîç O que faz?
Instala o Ollama no Ubuntu.
Inicia o servidor ollama serve.
Baixa automaticamente o modelo leve TinyLlama (1.1B) para infer√™ncia local.
Exp√µe a API em http://localhost:11434.
Usa volume Docker para n√£o baixar o modelo novamente em reinicializa√ß√µes.
‚úÖ O script oficial do Ollama detecta automaticamente GPU (NVIDIA/AMD) e instala drivers se necess√°rio. 

üåê Servi√ßo 2: web-container (Backend + Frontend)
Dockerfile
Dockerfile


1
2
3
4
5
6
7
8
9
10
11
12
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "app.py"]
requirements.txt
txt


1
2
flask
requests
app.py ‚Äì Backend Flask
Este script:

Detecta temas jur√≠dicos com base em palavras-chave (ex: "div√≥rcio", "FGTS", "aposentadoria").
Classifica a √°rea do direito mais relevante.
Consulta o modelo local tinyllama via API do Ollama.
Gera respostas humanizadas com chamadas para a√ß√£o (CTA).
Insere bot√µes do WhatsApp para convers√£o.
Redireciona sempre para contato humano ‚Äì nunca diz "n√£o posso ajudar".
Exemplo de CTA gerado:
html


1
2
3
‚åÑ
<a href="https://wa.me/551199887766?text=Quero+falar+sobre+div√≥rcio">
  üìû Falar com especialista em Direito de Fam√≠lia
</a>
üí¨ Funcionalidades do Chatbot (app.py)
O backend app.py possui m√∫ltiplas camadas de intelig√™ncia:

1. Detec√ß√£o de Inten√ß√£o
Sauda√ß√µes, despedidas, contato, hor√°rios, honor√°rios.
Temas comuns como "div√≥rcio", "acidente", "golpe no PIX".
Benef√≠cios previdenci√°rios com respostas espec√≠ficas.
2. Palavras-Chave por √Årea
python


1
2
3
4
5
6
‚åÑ
PALAVRAS_JURIDICAS = {
    "Direito de Fam√≠lia": ["div√≥rcio", "guarda", "pens√£o", ...],
    "Direito Trabalhista": ["demitido", "horas extras", "FGTS", ...],
    "Direito Previdenci√°rio": ["aposentadoria", "aux√≠lio-doen√ßa", ...],
    # + outras √°reas
}
3. IA com Foco em Convers√£o
Prompt otimizado para empathia e a√ß√£o.
Nunca responde como rob√¥.
Sempre termina com CTA para WhatsApp.
4. Redirecionamento Garantido
Mesmo se a IA falhar, o usu√°rio √© direcionado ao WhatsApp.
Nenhum caso √© perdido.
üñ•Ô∏è Frontend (index.html)
Caracter√≠sticas
Design moderno com Bootstrap 5 e Font Awesome.
Tipografia elegante com Google Fonts (Inter + Playfair Display).
Chatbot fixo no canto inferior direito com anima√ß√£o de pulsa√ß√£o.
Totalmente responsivo (m√≥vel e desktop).
Recursos do Chat
Abre com clique no bot√£o flutuante.
Interface limpa com mensagens do usu√°rio (azul) e IA (cinza).
Mostra "digitando..." enquanto aguarda resposta.
Envia mensagem com Enter.
Comunica√ß√£o com Backend
javascript


1
2
3
4
5
‚åÑ
fetch('http://localhost:5000/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ pergunta: msg })
})
üê≥ Docker Compose
docker-compose.yml
yaml


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
‚åÑ
‚åÑ
‚åÑ
‚åÑ
‚åÑ
‚åÑ
‚åÑ
‚åÑ
services:
  ollama:
    build: ./ollama-container
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  web:
    build: ./web-container
    ports:
      - "5000:5000"
    depends_on:
      - ollama

volumes:
  ollama_data:
Funcionalidades:
O servi√ßo web s√≥ inicia ap√≥s o ollama estar pronto.
Dados do Ollama s√£o persistidos no volume ollama_data.
Portas expostas: 5000 (site) e 11434 (API da IA).
üöÄ Como Executar
Clone o reposit√≥rio
Certifique-se de ter Docker e Docker Compose instalados
Execute:

docker-compose up --build

Acesse:
Site: http://localhost:5000
API da IA: http://localhost:11434
üí° Recursos de Convers√£o
O sistema foi projetado para maximizar convers√µes:

Primeira consulta gratuita
Reduz barreira inicial
Bot√µes do WhatsApp
Convers√£o direta
CTAs personalizados por √°rea
Maior relev√¢ncia
Plant√£o 24h
Urg√™ncia
Respostas curtas e humanas
Clareza e empatia
Redirecionamento para humano
Nenhum caso perdido

üõ°Ô∏è Privacidade e Seguran√ßa
Nenhum dado do usu√°rio √© armazenado.
Toda IA roda localmente (sem envio a nuvens como OpenAI).
Sem cookies de rastreamento.
Ideal para advocacia: cumple LGPD e √©tica profissional.
üß† Modelo de IA Utilizado
tinyllama:1.1b
Pequeno, r√°pido e eficiente.
Roda bem em CPU (ideal para produ√ß√£o).
Treinado em linguagem natural.
Adaptado via prompt para responder como um advogado emp√°tico.
‚úÖ Futuramente pode ser substitu√≠do por modelos maiores como llama3, phi3 ou gemma com melhor desempenho. 

üìà Fluxo de Atendimento
Usu√°rio digita: "Ca√≠ em golpe no PIX, o que fazer?"
Sistema detecta palavra-chave ‚Üí "golpe", "PIX"
Classifica como: Direito do Consumidor
Gera resposta com CTA:
"Se voc√™ foi enganado no PIX, podemos tentar recuperar seu dinheiro.
üìû Falar com especialista em Consumidor " 
Usu√°rio clica e conversa com advogado real.
üìû Contato
Este projeto foi desenvolvido para escrit√≥rios de advocacia que desejam:

Atendimento 24h
Redu√ß√£o de custos
Aumento de convers√£o
Entre em contato:
üìß contato@drlegal.com.br
üìû (11) 99999-9999

üìÑ Licen√ßa
Este projeto √© um exemplo educacional.
Voc√™ pode adapt√°-lo livremente para uso comercial ou institucional.